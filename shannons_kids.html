<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Pratyaksh Patel | Technical Blog</title>

  <!-- LaTeX.css -->
  <link rel="stylesheet" href="https://latex.vercel.app/style.css" />

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-69CER33QN8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-69CER33QN8');
  </script>

  <style>
    body {
      font-size: 1.1rem;
      max-width: 720px;
      margin: 0 auto;
      padding: 2rem 1rem;
      background: #f9f9f9;
      color: #333;
    }

    h1 {
      font-size: 1.6rem;
      margin-bottom: 1rem;
      text-align: center;
    }

    h2 {
      font-size: 1.3rem;
      margin-top: 2rem;
      margin-bottom: 0.6rem;
    }

    a:hover {
      font-size: 1.05em;
    }

    p {
      margin-bottom: 1rem;
    }

    @media (max-width: 500px) {
      body {
        font-size: 1rem;
        padding: 1rem;
      }

      h1 {
        font-size: 1.4rem;
      }

      h2 {
        font-size: 1.15rem;
      }
    }
  </style>
</head>

<body>
  <h1>Shannon's Kids</h1>
  <p>15 July 2025</p>

  <p>One of the things I love about IISERB is the annual <a href="https://sites.google.com/view/maths-club-iiser-bhopal/home?authuser=0" target="_blank">math club</a> fest of Continuum. This year, one of the events was a 7-minute symposium where you could present any topic — as long as you used mathematics. I chose Shannon's kids: KL and JS divergences. It won me second prize and a sipper, now employed for iced tea on lazy afternoons.</p>

  <p>Imagine two professors grading differently. BIO101 has Gaussian-like scores — balanced and symmetric. ECS201 or MTH202, however, reveal skewed results. It’s obvious visually — but how can we measure this difference rigorously and quantitatively? That's where information theory enters.</p>

  <h2>The Hassle: Uneven Grading Distributions</h2>
  <p>Here's what these distributions might look like:</p>
  <img src="output1.png" alt="It's bad, isn't it?" style="width:100%;"/>

  <p>Not all distributions describe reality equally well. KL divergence tells us how inefficient one distribution is in describing data generated from another. This inefficiency — in bits — is the cost of using the wrong model.</p>

  <h2>KL Divergence: The First, Asymmetric Kid</h2>
  <p>Kullback-Leibler divergence quantifies the discrepancy between two probability distributions:</p>

  <p>
    \[ D_{KL}(P || Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)} \]
  </p>

  <ul>
    <li>Measures the expected number of extra bits to encode samples from \(P\) using a code optimized for \(Q\).</li>
    <li>Not symmetric — it matters which distribution is the reference.</li>
    <li>Infinite if \(Q(x) = 0\) and \(P(x) > 0\), due to division by zero inside the logarithm.</li>
  </ul>

  <p>KL divergence is zero iff \(P = Q\) almost everywhere. In this sense, it acts as a quasi-distance measure on the space of distributions.</p>

  <h2>Shannon’s Legacy and KL Divergence</h2>
  <p>The KL divergence is rooted in Shannon’s definition of entropy, which measures the average uncertainty (or information content) in a distribution:</p>

  <p>
    \[ H(P) = - \sum_x P(x) \log P(x) \]
  </p>

  <p>Cross-entropy is the average number of bits needed to encode samples from \(P\) using a coding scheme optimized for \(Q\):</p>

  <p>
    \[ H(P, Q) = - \sum_x P(x) \log Q(x) \]
  </p>

  <p>The KL divergence is simply the difference:</p>

  <p>
    \[ D_{KL}(P || Q) = H(P, Q) - H(P) \]
  </p>

  <p>This makes its interpretation in coding theory precise: how many more bits are needed, on average, because you assumed the wrong distribution?</p>

  <h2>A Real Example</h2>
  <p>Suppose Biology marks are normally distributed and Math marks are skewed. Then:</p>

  <ul>
    <li>\(D_{KL}(\text{Bio} || \text{Math}) = 1.0034\)</li>
    <li>\(D_{KL}(\text{Math} || \text{Bio}) = 2.6962\)</li>
  </ul>

  <p>This tells us assuming Math is Bio is much worse than the reverse — there’s more inefficiency, more surprise.</p>

  <h2>Understanding Information</h2>
  <p>Think of guessing a number from 1 to 8. Each binary question like “Is it > 4?” gives 1 bit of information. Since there are 8 options, you need:</p>

  <p>
    \[ \log_2 8 = 3 \text{ bits} \]
  </p>

  <p><strong>Information content</strong> of an event with probability \(p\) is:</p>

  <p>
    \[ I(x) = -\log_2 p(x) \]
  </p>

  <p>Thus, rarer events contain more information. Shannon entropy is just the expected value of this surprise.</p>

  <h2>Why KL Isn’t Enough</h2>
  <p>While powerful, KL divergence has limitations:</p>
  <ul>
    <li><strong>Asymmetry:</strong> direction matters — switching \(P\) and \(Q\) gives different results.</li>
    <li><strong>Undefined for zeros:</strong> if \(Q(x) = 0\) and \(P(x) > 0\), then KL diverges to infinity.</li>
  </ul>

  <p>To address these, a more robust measure was developed.</p>

  <h2>Jensen-Shannon Divergence: The Symmetric One</h2>
  <p>Jensen-Shannon Divergence is a symmetrized and smoothed version of KL divergence:</p>

  <p>
    \[ D_{JS}(P || Q) = \frac{1}{2} D_{KL}(P || M) + \frac{1}{2} D_{KL}(Q || M), \quad M = \frac{1}{2}(P + Q) \]
  </p>

  <ul>
    <li>Symmetric by construction.</li>
    <li>Always finite — even if \(P\) or \(Q\) contain zeros.</li>
    <li>Bounded: \( 0 \leq D_{JS} \leq \log 2 \).</li>
  </ul>

  <h2>Entropy Formulation of JSD</h2>
  <p>JSD can also be written using entropies:</p>

  <p>
    \[ D_{JS}(P || Q) = H(M) - \frac{1}{2} H(P) - \frac{1}{2} H(Q) \]
  </p>

  <p>This measures how much more uncertain the mixture \(M\) is compared to the average uncertainty of \(P\) and \(Q\).</p>

  <p>It also has a nice interpretation: it is the mutual information between a sample and its origin label (whether it came from \(P\) or \(Q\)) — thus connecting information theory with classification.</p>

  <h2>Where I Learned This: Topic Modeling</h2>
  <p>In Latent Dirichlet Allocation (LDA), topics are distributions over words, and documents are mixtures of topics. KL divergence plays a central role:</p>
  <ul>
    <li><strong>Comparing topic similarity:</strong> are two topics semantically close?</li>
    <li><strong>Document inference:</strong> how likely is a document under a given topic model?</li>
    <li><strong>Optimization:</strong> LDA uses variational inference to minimize KL divergence between true and approximate posteriors.</li>
  </ul>

  <h2>Conclusion</h2>
  <p>KL divergence gave us a powerful lens for measuring divergence between beliefs and reality — but it’s not perfect. Jensen-Shannon builds on it with symmetry and robustness. Behind them all, Shannon's insight remains foundational: information is uncertainty, and math can measure it.</p>

  <section class="references">
    <h2>References</h2>
    <ol>
      <li>Kullback, S., & Leibler, R. A. (1951). On information and sufficiency. Annals of Mathematical Statistics.</li>
      <li>Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal.</li>
      <li>Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory.</li>
      <li>Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research.</li>
    </ol>
  </section>

</body>
</html>

