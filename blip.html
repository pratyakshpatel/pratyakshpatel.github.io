<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Pratyaksh Patel | Technical Blog</title>

  <!-- LaTeX.css -->
  <link rel="stylesheet" href="https://latex.vercel.app/style.css" />

  <!-- MathJax for LaTeX-style math -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-69CER33QN8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-69CER33QN8');
  </script>

</head>
<body>

<main>

<h1>BLIP: Bootstrapping Language-Image Pretraining for Unified Vision-Language Understanding</h1>
<p><em>5 July, 2025</em></p>

<p>
While I was working at IIMN this summer, I had an idea: to compare the original captions of social media images with what is actually happening in the image. In exploring this, I came across BLIP — by far, one of the coolest things I’ve read. It refreshed my whole course on Deep Learning.
</p>

<p>
In AI, the ability to jointly understand and generate visual and textual content has become an essential frontier. Models like CLIP and ALIGN showed that combining image and text modalities unlocks powerful capabilities like captioning, VQA, and retrieval [1]. However, these relied heavily on enormous noisy web datasets. To address this, <strong>BLIP</strong> (Bootstrapped Language-Image Pretraining) by Salesforce Research [2] proposed a more efficient and flexible approach.
</p>

<p>
BLIP is a unified vision-language framework that handles both understanding and generation. It uses three objectives: contrastive learning, image-text matching, and caption generation. A key novelty is the <em>Q-former</em> module — a Transformer with learnable query tokens that efficiently connect visual and textual features. Using this along with bootstrapped learning, BLIP trains with both real and synthetic data, improving continuously.
</p>

<h2>Architecture and Learning Mechanism</h2>

<p>
BLIP includes three parts: a vision encoder (usually ViT), a Q-former, and a text encoder/decoder. The vision encoder generates patch-level embeddings. Instead of passing all to the language model (which is inefficient), BLIP uses learnable query tokens via the Q-former to selectively attend to the important visual regions.
</p>

<p>
The text module works in two modes. For understanding tasks (e.g., matching), it acts like BERT. For generation (e.g., captioning), it behaves like GPT. This flexible design allows BLIP to excel at both modalities [2].
</p>

<h2>Training Objectives and Mathematical Foundations</h2>

<p>
BLIP is trained on three losses:
</p>

<h3>1. Image-Text Contrastive (ITC) Loss</h3>

<p>
Aligns image and text embeddings \( z_i \) and \( z_t \) in the same space. Using temperature parameter \( \tau \), the loss is:
</p>

\[
\mathcal{L}_{\text{ITC}} = -\log \left( \frac{\exp(\text{sim}(z_i, z_t)/\tau)}{\sum_j \exp(\text{sim}(z_i, z_{t_j})/\tau)} \right)
\]

<p>
This encourages matching pairs to be similar and non-matching to be dissimilar [2].
</p>

<h3>2. Image-Text Matching (ITM) Loss</h3>

<p>
A binary classification loss for correct image-text pairs:
</p>

\[
\mathcal{L}_{\text{ITM}} = -y \log(p) - (1 - y)\log(1 - p)
\]

<p>
where \( y \in \{0, 1\} \) is the label and \( p \) is the predicted match probability.
</p>

<h3>3. Captioning Loss</h3>

<p>
Cross-entropy loss for caption generation via teacher-forcing:
</p>

\[
\mathcal{L}_{\text{CAP}} = - \sum_t \log P(w_t \mid w_{1:t-1}, \text{image})
\]

<p>
This enables accurate generation of descriptive captions from visual input.
</p>

<h2>Bootstrapped Pretraining Strategy</h2>

<p>
BLIP uses its own captioning model to create captions for unlabeled images. These are filtered using the ITM head to ensure relevance. Valid pairs are re-used for further training. This bootstrapping loop creates an evolving self-learning process, helping BLIP scale without relying on massive manually-labeled datasets.
</p>

<p>
This self-supervised approach keeps the model efficient and compact while remaining competitive. It adapts well across tasks with little fine-tuning.
</p>

<h2>Conclusion</h2>

<p>
BLIP is a powerful step forward in multimodal AI. By unifying contrastive and generative pretraining under a flexible architecture with query-based visual grounding, it achieves performance rivaling larger models — without the overhead. Its design and efficiency make it a cornerstone for future vision-language systems.
</p>

<h2>References</h2>
<ol>
  <li>Radford, A., et al. (2021). <i>Learning Transferable Visual Models From Natural Language Supervision</i>. In ICML.</li>
  <li>Li, J., Li, D., Xiong, C., & Hoi, S. C. (2022). <i><a href="https://arxiv.org/abs/2201.12086" target="_blank">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding</a></i>. In ICML.</li>
  <li>Dosovitskiy, A., et al. (2021). <i>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</i>. In ICLR.</li>
  <li>Devlin, J., et al. (2018). <i>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</i>. In NAACL-HLT.</li>
</ol>

</main>
</body>
</html>

