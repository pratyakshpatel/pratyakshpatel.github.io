<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Pratyaksh Patel | Technical Blog</title>

  <!-- LaTeX.css -->
  <link rel="stylesheet" href="https://latex.vercel.app/style.css" />

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-69CER33QN8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);} 
    gtag('js', new Date());
    gtag('config', 'G-69CER33QN8');
  </script>

  <style>
    body {
      font-size: 1.1rem;
      max-width: 720px;
      margin: 0 auto;
      padding: 2rem 1rem;
      background: #f9f9f9;
      color: #333;
    }

    h1 {
      font-size: 1.8rem;
      margin-bottom: 1rem;
      text-align: center;
    }

    h2 {
      font-size: 1.4rem;
      margin-top: 2.5rem;
      margin-bottom: 0.8rem;
      padding-bottom: 0.3rem;
      border-bottom: 1px solid #ddd;
    }

    h3 {
      font-size: 1.2rem;
      margin-top: 1.5rem;
      margin-bottom: 0.6rem;
    }

    pre {
      background: #fff;
      padding: 0.75rem;
      border-radius: 6px;
      overflow-x: auto;
      box-shadow: 0 1px 0 rgba(0,0,0,0.04);
    }

    code { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", "Liberation Mono", monospace; }

    a:hover {
      text-decoration: underline;
    }

    p {
      line-height: 1.6;
      margin-bottom: 1rem;
    }
    
    blockquote {
      border-left: 3px solid #ccc;
      padding-left: 1rem;
      margin-left: 0;
      font-style: italic;
      color: #555;
    }

    footer {
      margin-top: 3rem;
      padding-top: 1.5rem;
      border-top: 1px solid #e6e6e6;
      font-size: 0.95rem;
      color: #555;
      text-align: center;
    }

    @media (max-width: 500px) {
      body {
        font-size: 1rem;
        padding: 1rem;
      }

      h1 {
        font-size: 1.5rem;
      }

      h2 {
        font-size: 1.25rem;
      }
    }
  </style>
</head>
<body>

<main>
  <h1>RisingBALLER — A Deep Dive into Treating Players as Tokens</h1>

  <article>
    <p>9 June, 2025</p>

    <p>
      Every now and then, a paper comes along that doesn't just present a new model, but a new way of thinking. I recently stumbled upon one such paper from the StatsBomb Conference 2024 that genuinely made me sit up and reconsider how we can apply modern AI to sports analytics. It's called <em>RisingBALLER</em>, and its core premise is as elegant as it is powerful: <strong>what if we treat football matches like sentences and players like tokens?</strong>
    </p>
    <p>
      This blog is my deep dive into that very idea. I'm going to walk you through the entire methodology, the math that underpins it, and the fascinating results, sharing my perspective on why I think this approach is so transformative. Everything here is sourced directly from the original paper—I'm just adding my own narrative as I explore their work.
    </p>

    <h2>The Core Idea: Football Through the Lens of NLP</h2>
    <p>
      The intuition behind RisingBALLER is what I found most captivating. The authors essentially asked: why can't we use the same foundation model concepts that revolutionized Natural Language Processing (NLP) for football? In NLP, a transformer model learns the meaning of words (tokens) by looking at their context within a sentence. RisingBALLER ports this idea directly to the pitch.
    </p>
    <blockquote>
      Each <strong>player</strong> in a match becomes a token, and the <strong>match</strong> itself becomes the sentence.
    </blockquote>
    <p>
      By feeding this sequence into a transformer, the model learns deeply contextualized player embeddings that are specific to that single match. This unlocks a whole host of downstream tasks, from predicting future performance and finding stylistically similar players to even estimating abstract concepts like team cohesion. It's a fundamental shift from static player attributes to dynamic, context-aware representations.
    </p>

    <h2>Building the Foundation: Data and Preprocessing</h2>
    <p>
      As any data scientist knows, an idea is only as good as the data it's built on. For this project, the authors used the incredible StatsBomb event dataset, focusing on the 2015–2016 season across the top 5 European leagues. The raw data for a single match consists of 3,500-4,000 event rows, which isn't directly usable by a transformer.
    </p>
    <p>
      So, the first crucial step was a heavy dose of preprocessing. I was impressed by how they converted this event stream into a structured, per-player statistics table for each match. Every player in the match day squad (both the starting XI and the bench) was given a feature vector. For their main downstream task, Next Match Statistics Prediction (NMSP), they didn't just use raw stats. They selected <strong>39 base statistics</strong> (like progressive passes, successful dribbles, aerial duels won, interceptions, xG, etc.) and then engineered aggregates (sums, means, and standard deviations over rolling 3 and 5-match windows) to create a rich, <strong>234-variable feature vector</strong> for each player. This captures not just what a player did in one match, but their recent form and consistency.
    </p>

    <h2>The Model Architecture: Deconstructing a "Player-Token"</h2>
    <p>
      Now, let's get into the technical core of RisingBALLER. The fundamental building block is how they represent each player within a match. For any given player, the model constructs four separate embeddings and then sums them element-wise. This creates a rich initial "token" embedding that captures multiple facets of the player's context.
    </p>
    <p>
      If we denote the embedding dimension by \(D\) and the number of players in the match sequence by \(N\) (the paper uses a fixed sequence length of up to 80, padding where necessary), then for a player \(i\), I can write their initial embedding like this:
    </p>
    <p>
      \[
        \mathbf{x}^{(i)}_{init} = \mathbf{e}^{(i)}_{player} + \mathbf{e}^{(i)}_{pos} + \mathbf{e}^{(i)}_{team} + \mathbf{e}^{(i)}_{tpe},
      \]
    </p>
    
    <h3>The Four Pillars of a Player Embedding</h3>
    <p>Let me break down what each of these components represents, as this is key to the whole model:</p>
    <ul>
      <li>\(\mathbf{e}_{player}\): This is the unique <strong>player-ID embedding</strong>. Think of this as the model learning an intrinsic signature for every single player, capturing their inherent skills and style, independent of context.</li>
      <li>\(\mathbf{e}_{pos}\): This is a <strong>spatial positional embedding</strong>. It tells the model about the player's general position on the pitch (e.g., center-back, attacking midfielder), giving it a tactical anchor.</li>
      <li>\(\mathbf{e}_{team}\): This is the <strong>team affiliation embedding</strong>. It captures the tactical system, coaching style, and overall quality of the team the player belongs to.</li>
      <li>\(\mathbf{e}_{tpe}\): This is what they call the <strong>temporal positional encoding</strong>. This is the clever part where the actual match statistics come in. The vector of stats is projected into the same \(D\)-dimensional space, grounding the player's identity in their concrete performance in that specific match.</li>
    </ul>
    
    <h3>The Transformer's Role</h3>
    <p>
      Once these initial embeddings are created for all \(N\) players, they're stacked into a matrix \(X_{init} \in \mathbb{R}^{N\times D}\). This matrix is then fed through a standard transformer encoder. It's here that the real magic happens. Through its multi-head self-attention mechanism, the transformer allows every player-token to 'look' at every other token in the sequence. The model learns who to pay attention to, effectively asking questions like "Given this midfielder's performance, how should I update my understanding of the striker he was passing to?" The output is a matrix of context-aware embeddings \(X_{out}\in\mathbb{R}^{N\times D}\), where each player's vector is now enriched with information about everyone else in that match.
    </p>

    <h2>Pre-training: Developing a "Football IQ"</h2>
    <p>
      To get the transformer to learn these complex relationships, the authors used a self-supervised pre-training task they call <strong>Masked Player Prediction (MPP)</strong>. If you're familiar with NLP models like BERT, this is directly analogous to Masked Language Modeling (MLM).
    </p>
    <p>
      For each match, they randomly hide (or "mask") 25% of the players in the input sequence. The model's job is to predict the identities of these masked players based on the context provided by the unmasked players. This forces the model to develop a deep "football IQ." For instance, if it sees a sequence of Real Madrid defenders and midfielders from 2016, and one player is masked, it has to learn that the missing player is likely to be someone like Cristiano Ronaldo, based on the surrounding context.
    </p>
    <p>
      From a technical perspective, for each masked position \(j\), the model takes the final contextualized output vector \(\mathbf{x}^{(j)}_{out}\) and projects it into a probability distribution over the entire vocabulary of players \(V\). This is done using a standard softmax layer:
    </p>
    <p>
      \[
        P(\hat{y}_j = v \mid X_{out}) = \mathrm{softmax}\left( W_{v}^\top \mathbf{x}^{(j)}_{out} + b_v\right) ,\quad v\in\{1,\dots,|V|\}.
      \]
    </p>
    <p>
      The model is then trained to minimize the cross-entropy loss, which essentially penalizes it for making wrong predictions. For a single match, the MPP loss function is:
    </p>
    <p>
      \[
        \mathcal{L}_{MPP} = -\sum_{j\in M} \log P(y_j\mid X_{out}).
      \]
    </p>

    <h2>Fine-tuning: From General Knowledge to Specific Prediction</h2>
    <p>
      After the pre-training phase, the model has a deep, contextual understanding of players. The next step, which I think demonstrates the real utility of this approach, is fine-tuning it for a specific downstream task: <strong>Next Match Statistics Prediction (NMSP)</strong>. They take the pre-trained transformer, remove the MPP head, and attach a new MLP head. This new head is trained to take the contextualized representations of a team's players and predict 18 team-level statistics for the *next* game.
    </p>
    <p>
      For this task, the model's goal is to predict the next-match team stats vector \(\mathbf{y}\in\mathbb{R}^{2N_{stats}}\) (one set of stats for each of the two teams). The training objective is a straightforward mean squared error, averaged across all the predicted statistics:
    </p>
    <p>
      \[
        \mathcal{L}_{NMSP} = \frac{1}{2N_{stats}} \sum_{k=1}^{2N_{stats}} (\hat{y}_k - y_k)^2.
      \]
    </p>
    <p>
      During fine-tuning, all the model's weights are updated. This allows the model to adapt its general football knowledge specifically to the task of statistical prediction, a process known as transfer learning.
    </p>

    <h2>So, Did It Work? The Key Results</h2>
    <p>
      This is the moment of truth. After all this clever setup, does the model actually perform well? Based on the paper, the answer is a resounding yes. Here's my summary of their key findings:
    </p>
    <ul>
      <li><strong>MPP Performance:</strong> On the pre-training task, both the 64-D and 128-D models achieved a top-3 accuracy of over 95%. To me, this is a strong signal that the model isn't just memorizing; it's genuinely learning meaningful player roles and relationships.</li>
      <li><strong>NMSP Performance:</strong> When fine-tuned, the RisingBALLER models significantly outperformed a tough baseline (which predicted stats as the average of the previous five matches). The 128-D model reduced the global average MSE by about <strong>37.7%</strong>, and the 64-D model by <strong>35.35%</strong>. That's a substantial improvement.</li>
      <li><strong>Ablation Studies:</strong> I always appreciate good ablation studies. The authors found that removing the team-affiliation embeddings caused the MPP top-3 accuracy to plummet from ~95% to ~82%, confirming that team context is a very strong signal. This tells us the model heavily relies on knowing who a player's teammates are.</li>
    </ul>

    <h2>Beyond Prediction: Unlocking the Player Embeddings</h2>
    <p>For me, this is the most exciting part of the paper. The predictive accuracy is great, but the true power of this approach lies in the learned embeddings themselves. They are rich, nuanced representations of players that can be used for all sorts of analysis. The authors showcase a few fantastic examples:</p>
    <ol>
      <li><strong>Positional Clustering:</strong> When they visualized the learned positional embeddings, they found that the vectors naturally clustered into defenders, midfielders, and attackers. This shows the model learns the tactical structure of a football pitch organically.</li>
      <li><strong>Similar Player Retrieval:</strong> This is a classic use case with a new twist. By finding the nearest neighbors to a player's embedding, you can find others who perform a similar role. When I saw their example of querying for players similar to the 2016 version of N'Golo Kanté, the results were stunning. It didn't just find other defensive midfielders; it found players with a similar "engine" and defensive work-rate, like Idrissa Gueye and Allan, even across different leagues.</li>
      <li><strong>A Metric for Team Cohesion:</strong> The authors proposed a fascinating heuristic for team cohesion: calculate the average pairwise similarity (e.g., cosine similarity) of all players in a team's starting lineup. A higher score might indicate a more stylistically coherent unit that "sings from the same hymn sheet."</li>
      <li><strong>Attention Analysis:</strong> While noted as future work, one could analyze the transformer's attention matrices to see which players "pay attention" to which other players, potentially revealing on-pitch synergies, like the connection between a playmaker and a striker.</li>
    </ol>
    
    <h2>A Frank Look at Limitations and Future Directions</h2>
    <p>
      Of course, no model is perfect, and the authors are transparent about the limitations.
    </p>
    <p>
      <strong>Its key strengths, in my view, are:</strong>
    </p>
    <ul>
      <li>It automates the discovery of complex, contextual player attributes, moving away from laborious handcrafted feature engineering.</li>
      <li>The pre-training/fine-tuning paradigm makes the model incredibly flexible and adaptable to many different downstream tasks.</li>
      <li>The learned embeddings provide a rich, quantitative foundation for previously qualitative analysis, like player similarity and team style.</li>
    </ul>
    <p>
      <strong>However, there are important caveats:</strong>
    </p>
    <ul>
      <li>The single-season dataset is relatively small, risking the model overfitting to specific team tactics of that season.</li>
      <li>The model itself is tiny by modern standards. I'm very curious to see what would happen if this approach were scaled up with more data and larger models.</li>
      <li>They note that predicting high-variance stats like goals was still very difficult. This makes sense—goals are rare events often influenced by luck, making them inherently harder to predict than high-volume stats like passes.</li>
    </ul>

    <h2>My Final Take</h2>
    <p>
      RisingBALLER isn't just another model; I see it as a blueprint for the future of sports analytics. It demonstrates that the principles of modern AI—self-supervised pre-training on large datasets followed by task-specific fine-tuning—are just as powerful on the football pitch as they are in natural language. It moves the field from static analysis to a dynamic, context-aware understanding of players and teams. I, for one, am incredibly excited to see where this line of research goes next.
    </p>

    <footer>
      <p>
        Source paper: RisingBALLER — A Player is a Token, A Match is a Sentence (StatsBomb Conference 2024, Research Stage). 
      </p>
    </footer>

  </article>
</main>

</body>
</html>
